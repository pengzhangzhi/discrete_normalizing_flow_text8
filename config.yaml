# TarFlow Text8 Training Configuration

# Data
data:
  root: ./data/text8
  batch_size: 512
  seq_len: 256
  num_workers: 4

# Model (shared width, different depth)
model:
  vocab_size: 27
  hidden_dim: 768
  noise_std_ratio: 0.0    # Noise as fraction of u.std() (0.0 = disabled, 0.1 = 10% noise)

# MLM (BERT-style masking)
mlm:
  enabled: true           # Enable MLM auxiliary loss
  mask_ratio: 0.15        # Fraction of tokens to mask (standard BERT = 0.15)
  weight: 1.0             # Loss weight: total_loss = flow_loss + weight * mlm_loss

# Encoder depth
encoder:
  n_layers: 2
  n_heads: 8

# Flow depth
flow:
  n_blocks: 12
  layers_per_block: 1

# Training
train:
  lr: 3.0e-5
  min_lr_ratio: 0.1       # Cosine decays to lr * min_lr_ratio
  warmup_steps: 10000     # Linear warmup steps
  weight_decay: 1.0e-3
  dropout: 0.0
  max_steps: 500000
  grad_clip: 1.0
  val_check_interval: 10000
  log_every_n_steps: 10
  precision: bf16-mixed
  seed: 42

# DDP / Multi-GPU
distributed:
  strategy: auto
  devices: auto
  num_nodes: 1

# Logging & Checkpoints
logging:
  project: tarflow-text8
  run_name: null
  save_dir: ./checkpoints  # Auto-resumes from last.ckpt if exists

# Generator (Stage B) - only used by train_generator.py
generator:
  encoder_ckpt: null        # Path to frozen Stage A encoder checkpoint (required)
  align_weight: 1.0         # Weight for representation alignment loss
