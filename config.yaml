# TarFlow Text8 Training Configuration

# Data
data:
  root: ./data/text8
  batch_size: 512
  seq_len: 256
  num_workers: 4

# Model (shared width, different depth)
model:
  vocab_size: 27
  hidden_dim: 768

# Encoder depth
encoder:
  n_layers: 2
  n_heads: 8

# Flow depth
flow:
  n_blocks: 12
  layers_per_block: 1

# Training
train:
  lr: 1.0e-4
  min_lr_ratio: 0.1       # Cosine decays to lr * min_lr_ratio
  warmup_steps: 1000      # Linear warmup steps
  weight_decay: 4e-4
  dropout: 0.0
  noise_std: 0.0          # Noise injection to encoder output (0.0 = disabled)
  max_steps: 1000000
  grad_clip: 1.0
  val_check_interval: 10000
  log_every_n_steps: 10
  precision: bf16-mixed
  seed: 42

# DDP / Multi-GPU
distributed:
  strategy: auto
  devices: auto
  num_nodes: 1

# Logging & Checkpoints
logging:
  project: tarflow-text8
  run_name: null
  save_dir: ./checkpoints  # Auto-resumes from last.ckpt if exists
