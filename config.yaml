# TarFlow Text8 Training Configuration

# Data
data:
  root: ./data/text8
  batch_size: 512
  seq_len: 256
  num_workers: 4

# Encoder (BERT-style)
encoder:
  vocab_size: 27
  hidden_dim: 256
  output_dim: 64
  n_layers: 2
  n_heads: 8

# TarFlow (GPT2-Small scale)
flow:
  hidden_dim: 768
  n_blocks: 12
  layers_per_block: 1

# Training
train:
  lr: 1.0e-4
  min_lr_ratio: 0.1       # Cosine decays to lr * min_lr_ratio
  warmup_steps: 1000      # Linear warmup steps
  weight_decay: 0.0
  dropout: 0.0
  max_steps: 1000000
  grad_clip: 1.0
  val_check_interval: 1000
  log_every_n_steps: 100
  precision: bf16-mixed
  seed: 42

# DDP / Multi-GPU
distributed:
  strategy: auto
  devices: auto
  num_nodes: 1

# Logging & Checkpoints
logging:
  project: tarflow-text8
  run_name: null
  save_dir: ./checkpoints  # Auto-resumes from last.ckpt if exists
